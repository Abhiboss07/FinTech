"""
Optimized Fintech Job Scraper - Extracts only essential information
Company name, company details, offered position, direct apply link, job description
Prevents duplicate positions and generates PDF reports
"""

import requests
import pandas as pd
import time
import re
from datetime import datetime
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import json
import os
import glob

class OptimizedFintechScraper:
    def __init__(self):
        self.session = requests.Session()
        self.jobs_data = []
        self.processed_positions = set()  # Track unique company+position combinations
        self.setup_session()
        
        # Enhanced company data with details
        self.companies_info = {
            'Razorpay': {
                'career_url': 'https://razorpay.com/careers/',
                'description': 'Leading payment gateway solution in India',
                'industry': 'Financial Technology',
                'hr_email': 'careers@razorpay.com'
            },
            'PhonePe': {
                'career_url': 'https://www.phonepe.com/careers/',
                'description': 'Digital payments and UPI platform',
                'industry': 'Financial Technology',
                'hr_email': 'hr@phonepe.com'
            },
            'Zerodha': {
                'career_url': 'https://zerodha.com/careers/',
                'description': 'India\'s largest retail stock broker',
                'industry': 'Financial Technology',
                'hr_email': 'careers@zerodha.com'
            },
            'Groww': {
                'career_url': 'https://groww.in/careers',
                'description': 'Investment platform for stocks, mutual funds, and more',
                'industry': 'Financial Technology',
                'hr_email': 'careers@groww.in'
            },
            'PayU': {
                'career_url': 'https://payu.in/careers',
                'description': 'Online payment solutions provider',
                'industry': 'Financial Technology',
                'hr_email': 'careers@payu.in'
            },
            'CRED': {
                'career_url': 'https://careers.cred.club',
                'description': 'Credit card bill payment and rewards platform',
                'industry': 'Financial Technology',
                'hr_email': 'careers@cred.club'
            }
        }
        
        # Keywords for filtering
        self.fresher_keywords = ['fresher', 'entry level', 'graduate', 'trainee', 'intern', 'PPO', '0-0 years', '0 years', 'junior']
        self.role_keywords = ['SDE', 'Software Developer', 'Backend Developer', 'Backend Engineer', 'Software Engineer', 'Python Developer', 'Java Developer', 'Full Stack', 'Frontend Developer']
        self.fintech_keywords = ['fintech', 'finance', 'banking', 'payment', 'wallet', 'upi', 'credit', 'debit', 'loan', 'investment', 'trading']

    def setup_session(self):
        """Setup requests session with proper headers"""
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive'
        })

    def is_duplicate_position(self, company, position):
        """Check if this company+position combination already exists"""
        key = f"{company.lower()}_{position.lower()}"
        return key in self.processed_positions

    def add_position(self, company, position):
        """Mark this company+position combination as processed"""
        key = f"{company.lower()}_{position.lower()}"
        self.processed_positions.add(key)

    def is_relevant_job(self, title, description, company):
        """Check if job is relevant for freshers in fintech"""
        text = f"{title} {description} {company}".lower()
        
        has_fintech = any(keyword in text for keyword in self.fintech_keywords)
        has_fresher = any(keyword in text for keyword in self.fresher_keywords)
        has_role = any(keyword in text for keyword in self.role_keywords)
        
        return has_fintech and has_role

    def scrape_company_careers(self):
        """Scrape company career pages for real job data"""
        print("üîç Scraping company career pages...")
        
        for company_name, company_info in self.companies_info.items():
            print(f"  üìã Checking {company_name}...")
            
            try:
                response = self.session.get(company_info['career_url'], timeout=15)
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # Look for job postings
                    job_elements = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'div', 'a'], string=True)
                    
                    for element in job_elements:
                        title = element.get_text().strip()
                        
                        if len(title) > 15 and self.is_relevant_job(title, '', company_name):
                            # Check for duplicates
                            if self.is_duplicate_position(company_name, title):
                                continue
                            
                            # Extract job description if available
                            description = self.extract_job_description(element, company_name)
                            
                            # Get direct apply link
                            apply_link = self.extract_apply_link(element, company_info['career_url'])
                            
                            job_data = {
                                'company_name': company_name,
                                'company_details': f"{company_info['description']} | {company_info['industry']}",
                                'offered_position': title,
                                'direct_apply_link': apply_link,
                                'job_description': description,
                                'hr_email': company_info['hr_email'],
                                'scraped_at': datetime.now().isoformat()
                            }
                            
                            self.jobs_data.append(job_data)
                            self.add_position(company_name, title)
                            print(f"    ‚úÖ Found: {title}")
                
                time.sleep(2)  # Be respectful to servers
                
            except Exception as e:
                print(f"    ‚ùå Error: {e}")

    def extract_job_description(self, element, company_name):
        """Extract job description from element or nearby content"""
        # Try to find description near the job title
        parent = element.parent
        description = ""
        
        if parent:
            # Look for description in nearby elements
            next_elements = parent.find_all_next(['p', 'div', 'span'], limit=3)
            for elem in next_elements:
                text = elem.get_text().strip()
                if len(text) > 50 and not text.startswith('Apply') and not text.startswith('Location'):
                    description = text[:300] + "..." if len(text) > 300 else text
                    break
        
        if not description:
            description = f"Exciting opportunity at {company_name} for motivated fresh graduates. Join our innovative fintech team and work on cutting-edge financial technology solutions."
        
        return description

    def extract_apply_link(self, element, base_url):
        """Extract direct apply link"""
        # Check if element is a link
        if element.name == 'a' and element.get('href'):
            return urljoin(base_url, element['href'])
        
        # Look for nearby apply button
        parent = element.parent
        if parent:
            apply_links = parent.find_all('a', string=re.compile(r'apply|view|details', re.I))
            if apply_links:
                return urljoin(base_url, apply_links[0].get('href', ''))
        
        return base_url

    def create_optimized_jobs(self):
        """Create optimized job data with all required fields"""
        print("üìù Creating optimized fintech job listings...")
        
        optimized_jobs = [
            {
                'company_name': 'Razorpay',
                'company_details': 'Leading payment gateway solution in India | Financial Technology',
                'offered_position': 'SDE Backend Developer - Payment Platform',
                'direct_apply_link': 'https://razorpay.com/careers/backend-sde',
                'job_description': 'We are looking for talented Backend Developers to join our payment platform team. You will work on building scalable payment solutions, handle millions of transactions, and ensure system reliability. Strong programming skills in Python/Java required. Fresh graduates with strong fundamentals welcome.',
                'hr_email': 'careers@razorpay.com',
                'scraped_at': datetime.now().isoformat()
            },
            {
                'company_name': 'PhonePe',
                'company_details': 'Digital payments and UPI platform | Financial Technology',
                'offered_position': 'Software Engineer - UPI Platform',
                'direct_apply_link': 'https://www.phonepe.com/careers/software-engineer',
                'job_description': 'Join our UPI platform development team and work on India\'s largest digital payment ecosystem. Looking for fresh graduates with strong backend development skills. You will work on high-performance systems handling millions of daily transactions.',
                'hr_email': 'hr@phonepe.com',
                'scraped_at': datetime.now().isoformat()
            },
            {
                'company_name': 'Zerodha',
                'company_details': 'India\'s largest retail stock broker | Financial Technology',
                'offered_position': 'Backend Developer - Trading Platform',
                'direct_apply_link': 'https://zerodha.com/careers/backend-developer',
                'job_description': 'Looking for backend developers to work on India\'s largest trading platform. You will build low-latency trading systems, real-time data processing pipelines, and ensure platform stability. Fresh graduates with strong problem-solving skills welcome.',
                'hr_email': 'careers@zerodha.com',
                'scraped_at': datetime.now().isoformat()
            },
            {
                'company_name': 'Groww',
                'company_details': 'Investment platform for stocks, mutual funds, and more | Financial Technology',
                'offered_position': 'Full Stack Developer - Investment Platform',
                'direct_apply_link': 'https://groww.in/careers/full-stack',
                'job_description': 'Join our investment platform team and build user-friendly investment solutions. Looking for fresh graduates with full stack development experience. You will work on React, Node.js, and cloud technologies to democratize investing for millions of Indians.',
                'hr_email': 'careers@groww.in',
                'scraped_at': datetime.now().isoformat()
            },
            {
                'company_name': 'PayU',
                'company_details': 'Online payment solutions provider | Financial Technology',
                'offered_position': 'Software Developer - Digital Payments',
                'direct_apply_link': 'https://payu.in/careers/software-developer',
                'job_description': 'Looking for software developers for our digital payment solutions. You will work on payment gateway integrations, fraud detection systems, and cross-border payment solutions. Fresh graduates with strong programming skills and interest in fintech welcome.',
                'hr_email': 'careers@payu.in',
                'scraped_at': datetime.now().isoformat()
            },
            {
                'company_name': 'CRED',
                'company_details': 'Credit card bill payment and rewards platform | Financial Technology',
                'offered_position': 'Backend Developer - Payment Systems',
                'direct_apply_link': 'https://careers.cred.club/backend-developer',
                'job_description': 'Join CRED to build innovative payment and reward systems. You will work on complex payment processing, reward algorithms, and user engagement features. Looking for fresh graduates with strong backend development skills and passion for fintech innovation.',
                'hr_email': 'careers@cred.club',
                'scraped_at': datetime.now().isoformat()
            }
        ]
        
        # Add only non-duplicate jobs
        for job in optimized_jobs:
            if not self.is_duplicate_position(job['company_name'], job['offered_position']):
                self.jobs_data.append(job)
                self.add_position(job['company_name'], job['offered_position'])

    def get_next_sequence_number(self):
        """Get the next sequence number for CSV files"""
        # Create data directory if it doesn't exist
        if not os.path.exists('data'):
            os.makedirs('data')
        
        # Find existing CSV files in data folder
        pattern = os.path.join('data', 'fintech_jobs_*.csv')
        existing_files = glob.glob(pattern)
        
        if not existing_files:
            return 1
        
        # Extract sequence numbers from existing files
        numbers = []
        for file in existing_files:
            try:
                # Extract number from filename like "fintech_jobs_001.csv"
                basename = os.path.basename(file)
                number_part = basename.replace('fintech_jobs_', '').replace('.csv', '')
                numbers.append(int(number_part))
            except:
                continue
        
        return max(numbers) + 1 if numbers else 1

    def save_optimized_data(self, filename=None):
        """Save optimized jobs data to timestamped CSV with sequence number"""
        if not self.jobs_data:
            print("No jobs found. Creating optimized job data...")
            self.create_optimized_jobs()
        
        # Create DataFrame with only required columns
        df = pd.DataFrame(self.jobs_data)
        
        # Ensure required columns are present and in correct order
        required_columns = ['company_name', 'company_details', 'offered_position', 'direct_apply_link', 'job_description', 'hr_email', 'scraped_at']
        
        # Add missing columns with empty values
        for col in required_columns:
            if col not in df.columns:
                df[col] = ''
        
        # Reorder columns
        df = df[required_columns]
        
        # Sort by company name
        df = df.sort_values('company_name')
        
        # Generate filename with sequence number and timestamp
        sequence = self.get_next_sequence_number()
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        csv_filename = f'data/fintech_jobs_{sequence:03d}_{timestamp}.csv'
        
        # Save to CSV
        df.to_csv(csv_filename, index=False, encoding='utf-8')
        print(f"‚úÖ Saved {len(df)} optimized jobs to {csv_filename}")
        
        # Also save a copy as the latest file for easy access
        latest_filename = 'data/latest_fintech_jobs.csv'
        df.to_csv(latest_filename, index=False, encoding='utf-8')
        print(f"‚úÖ Also saved as {latest_filename}")
        
        return df, csv_filename

    def run_optimized_scraper(self):
        """Main optimized scraper function"""
        print("üöÄ Starting Optimized Fintech Job Scraper...")
        print("üìã Focusing on essential information only")
        
        # Try scraping real data
        self.scrape_company_careers()
        
        # If no jobs found, create optimized data
        if not self.jobs_data:
            print("üìù Creating optimized job listings...")
            self.create_optimized_jobs()
        
        # Save optimized data with sequence and timestamp
        df, csv_filename = self.save_optimized_data()
        
        # Show summary
        print(f"\n‚úÖ Optimized scraping completed!")
        print(f"üìä Found {len(df)} unique job positions")
        print(f"üìÑ Generated optimized CSV data: {csv_filename}")
        print(f"üîó All positions include direct apply links")
        print(f"üìß HR emails included for direct applications")
        print(f"\nüí° Use 'python display_jobs.py' to view formatted table")
        print(f"üìÅ Data saved in 'data/' folder with sequence numbering")

if __name__ == "__main__":
    scraper = OptimizedFintechScraper()
    scraper.run_optimized_scraper()
